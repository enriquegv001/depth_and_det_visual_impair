# Monocular Deep Learning Multimodal with Object Relevance Estimation for Real-Time Navigation of Visually Impaired individuals (MMOR)
Real time Deep Learning assistant for visually impaired people. The model architecture fusion panoptic segmentation Panoptic FPN and monocular depth estimation Midas. The outcome is a video captured on a mobile device, generating spoken descriptions of user's environment to facilitate navigation, applying a heuristic algorithm for adapting prediction to user environment expectation. The model has been tested on members from Asociación Cultural y Recreativa para la Proyección del Invidente Puebla, A.C. (ACRIP) and result effective for user experience analysis.

## Table of Contents
- [Requirements](#Requirements)
- [Installation](#installation)
- [Usage](#usage)
- [Credits](#credits)
<!-- - [License](#license)
- [Badges](#badges)-->

## Requirements
  Python >= 3.7
  
  Run with GPU accelerator
  
  DroidCam >= 6.5.2
  
## Installation
1. Download DroidCam Client on [Widows](https://www.dev47apps.com/droidcam/windows/), (Mac or Linux)[https://www.dev47apps.com/droidcam/linux/].
2. Download DroinCam - WebCam app on [your smartphone](https://www.dev47apps.com/)
   
<!-- -->

## Usage
1. Connect DroidCam Client from a computer to your smartphone, by connect both devices to same WiFi > copy from smartphone to laptop the Decive IP and DroidCam Port > Start
2. Run [Colab Notebook](https://colab.research.google.com/drive/1OOsR4P0-gFLwfMYOtO4S5ga_LHOsWUo6?usp=sharing) to learn about basic usage.
   
## Credits

- Enrique García **Contact:** enriquegv001@gmail.com
- Rafael Espinosa **Contact:** rafael.espinosa.castaneda@tec.mx

<!--## License

This project is licensed under the [License Name] License - see the [LICENSE](LICENSE) file for details.


## Badges

[![License](https://img.shields.io/badge/License-[License Code]-blue.svg)](LICENSE)
-->

